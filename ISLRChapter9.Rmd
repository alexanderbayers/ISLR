---
title: "ISLR Chapter 9"
author: "Alex Bayers"
date: "11/24/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

###Exercise 1
```{r}
curve(3*x + 1, from = -10, to =10, col = "black")
curve(-.5*x + 1, from = -10, to =10, col = "red", add = TRUE)

```

The area below the red line is the area for which -2 + x(1) + 2x(2) < 0.  The area below the black line is the area for which 1 + 3x(1) - x(2) > 0.

###Exercise 2
#####A & B
```{r}
plot(x=NULL, y = NULL, xlim = c(-3, 1), ylim = c(0, 4), xlab = "x1", ylab = "x2")
z <- seq(from = 0, to = 2*pi, by = .01)
lines(x = - 1 +2*cos(z), y = 2 + 2*sin(z))
```

This is a circle centered at (-1, 2) with a radius of 2.  The area inside of and on the circle is the area is less than 4.  

#####C
(0, 0) is blue.  (-1, 1) is red.  (2, 2) is blue.  (3, 8) is blue.

#####D
This is simply an extended basis for the function.

###Exercise 3
#####A & B
```{r}
x1 <- c(3, 2, 4, 1, 2, 4, 4)
x2 <- c(4, 2, 4, 4, 1, 3, 1)
color <- c("red", "red", "red", "red", "blue", "blue", "blue")
print(data.frame(x1, x2, color))
plot(x = x1, y = x2, col = color, asp = 1, xlim = c(1, 4), ylim = c(1, 4), pch = 16)
x.line <- seq(from = 0, to = 5, by = 1)
lines(x = x.line, y = x.line - .5, col = "black")
```

The mathematical equation is  $x_{1}-x_{2} - .5 = 0$

#####C
Our rule is to classify to red if  $x_{1}-x_{2} - .5 < 0$, otherwise classify to blue.

#####D & E
```{r}
plot(x = x1, y = x2, col = color, asp = 1, xlim = c(1, 4), ylim = c(1, 4), pch = 16)
lines(x = x.line, y = x.line - .5, col = "black")
lines(x = x.line, y = x.line - 1, col = "blue", lty = 2)
lines(x = x.line, y = x.line, col = "red", lty = 2)
arrows(x0 = 2, y0 = 1, x1 = 1.75, y1 = 1.25, col = "blue", length = .1)
arrows(x0 = 4, y0 = 3, x1 = 3.75, y1 = 3.25, col = "blue", length = .1)
arrows(x0 = 2, y0 = 2, x1 = 2.25, y1 = 1.75, col = "red", length = .1)
arrows(x0 = 4, y0 = 4, x1 = 4.25, y1 = 3.75, col = "red", length = .1)
paste("The margin is", round((2*.25^2)^.5, 3))
```

#####F
The seventh point is a blue point located at (4, 1).  This is not a support vector, and therefore can be moved freely as long as it is less than .36 away from the line.

#####G
```{r}
plot(x = x1, y = x2, col = color, asp = 1, xlim = c(1, 4), ylim = c(1, 4), pch = 16)
lines(x = x.line, y = .9*x.line - .5, col = "black")
```

#####H
```{r}
x1 <- c(x1, 2)
x2 <- c(x2, 3)
color <- c(color, "blue")
plot(x = x1, y = x2, col = color, asp = 1, xlim = c(1, 4), ylim = c(1, 4), pch = 16)
lines(x = x.line, y = x.line - .5, col = "black")
```

###4
```{r}
library(e1071)
set.seed(1071)
x1 <- runif(200, min = 0, max = 10)
x2 <- runif(200, min = 0, max = 10)
color <- ifelse((x1 -4)^2 + (x2-3)^2 < 9, "red", "blue")
x1 <- x1 + rnorm(200, 0, 1)
x2 <- x2 + rnorm(200, 0, 1)
dat = data.frame(x1, x2, color = as.factor(color))
dat.train <- dat[1:100,]
dat.test <- dat[101:200,]
plot(dat.train$x1, dat.train$x2, xlim = c(-2, 12), ylim = c(-2, 12), col = as.character(dat.train$color), pch = 16)
```
Our generative model is we sample from a uniform distribution from 0 to 10, and everything in a circle of radius 3 around (4, 3) is red, otherwise its blue.  We then perturb this with normal noise with a standard deviation of 1.

```{r}
#Fit a linear model
set.seed(341)
tune.out=tune(svm,color~.,data=dat.train,kernel="linear", ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100), scale = TRUE))
summary(tune.out)
bestmod=tune.out$best.model
summary(bestmod)
plot(bestmod , dat.train)

radial.out=tune(svm,color~.,data=dat.train,kernel="radial", ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100), gamma = c(0.001, 0.01, 0.1, 1,5,10,100), scale = TRUE))
summary(radial.out)
bestmod.radial=radial.out$best.model
summary(bestmod.radial)
plot(bestmod.radial , dat.train)

poly.out=tune(svm,color~.,data=dat.train,kernel="polynomial", ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100), degree = c(2, 3, 4), scale = TRUE))
summary(poly.out)
bestmod.poly=poly.out$best.model
summary(bestmod.poly)
plot(bestmod.poly , dat.train)

linearpredict <- predict(bestmod, newdata = dat.test)
radialpredict <- predict(bestmod.radial, newdata = dat.test)
polypredict <- predict(bestmod.poly, newdata = dat.test)

lineartable <- table(dat.test$color, linearpredict)
radialtable <- table(dat.test$color, radialpredict)
polytable <- table(dat.test$color, polypredict)

print(lineartable)
print(radialtable)
print(polytable)


#TODO - build cross validation models for polynomial and radial basis kernels, compare test set error.
```

In the training set, a linear kernel has an error rate of 19% using the optimal parameters.  Using a polynomial model, we have an error rate of 26% using a cubic kernel.  Using a radial model, we obtain an error rate of only 10%.

The test set confirms this data quite well.  The linear model has an error rate of 22%, the polynomial 34%, and the radial 9%.  

###Exercise 5
#####A & B
```{r}
set.seed(197)
x1=runif(500, min = -10, max = 10)-0.5
x2=runif(500, min = -10, max = 10)-0.5
y=1*(x1^2-x2^2 > 0)
color = ifelse(y == 1, "blue", "red")
ex5.train <- data.frame(x1, x2, y, color)
plot(x=x1, y = x2, col=color, pch = 16)
```

#####C
```{r}
logistic.model <- glm(y ~ x1 + x2, data = ex5.train)
summary(logistic.model)
```

#####D
```{r}
pred.y <- predict(logistic.model, newdata = ex5.train, type = "response")
color.pred = ifelse(pred.y > .5, "blue", "red")
plot(x=x1, y = x2, col=color.pred, pch = 16)
```

#####E
```{r}
logistic.model.exp  <- glm(y ~ I(x1^2) + I(x2^2), data = ex5.train)
summary(logistic.model.exp)
```

#####F
```{r}
pred.exp <- predict(logistic.model.exp, newdata = ex5.train, type = "response")
color.exp = ifelse(pred.exp > .5, "blue", "red")
plot(x=x1, y = x2, col=color.exp, pch = 16)
```

#####G
```{r}
tune.linear=tune(svm,y~.-color,data=ex5.train,kernel="linear", ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100), scale = TRUE))
bestmodel.linear <- tune.linear$best.model
summary(bestmodel.linear)
pred.linear <- predict(bestmodel.linear, newdata = ex5.train, type = "response")
color.linsvm = ifelse(pred.linear > .5, "blue", "red")
plot(x=x1, y = x2, col=color.linsvm, pch = 16)
```

#####H
```{r}
tune.quad=tune(svm,y~.-color,data=ex5.train,kernel="polynomial", ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100), scale = TRUE, degree = c(2)))
bestmodel.quadratic <- tune.quad$best.model
summary(bestmodel.quadratic)
pred.quad <- predict(bestmodel.quadratic, newdata = ex5.train, type = "response")
color.linsvm = ifelse(pred.quad > .5, "blue", "red")
plot(x=x1, y = x2, col=color.linsvm, pch = 16)
```

#####I
The linear kernel is unable to capture the nonlinearities in the data.  By comparison, the quadratic model is.  However, despite that, the expanded basis for logistic regression does a better job capturing the nonlinearities than the quadratic svm.

###Exercise 6
#####A
```{r}
x1=runif(100)
x2=runif(100)
y <- ifelse(x1 > x2, 1, 0)
color <- ifelse(y==1, "blue", "red")
plot(x1, x2, col = color, pch = 16)
abline(a = 0, b =1, col = "black")
my.frame <- data.frame(x1, x2, y)
```

#####B
```{r}
tune.linear <- tune(svm, y ~., data=my.frame,kernel="linear", ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune.linear)
```

We achieve our best performance with a cost parameter of .01, with an 8.6% error rate.

#####C
```{r}
x1.test=runif(1000)
x2.test=runif(1000)
y.test <- ifelse(x1.test > x2.test, 1, 0)
new.frame <- data.frame (x1 = x1.test, x2 = x2.test, y = y.test)
test.matrix <- as.matrix(c(0.005, 0.01, 0.1, 1,5,10,100))
for (i in 1:length(test.matrix)){
  svm.model <- svm(y ~., data = my.frame, kernel = "linear", cost = test.matrix[i])
  svm.pred <- predict(svm.model, newdata = new.frame)
  svm.cat <- ifelse(svm.pred > 0.5, 1, 0)
  svm.table <- table(y.test, svm.cat)
  cat.error <- (svm.table[2,1] + svm.table[1, 2])/1000
  print(cat.error)
}

```


.1 minimizes the cost.  

###Exercise 7
#####A & B
```{r}
library(ISLR)
median.mpg <- median(Auto$mpg)
Auto$MPGBin <- ifelse(Auto$mpg > median.mpg, 1, 0)
tune.linear=tune(svm,MPGBin~.-mpg,data=Auto,kernel="linear", ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100), scale = TRUE))
summary(tune.linear)
```

Error is minimized with a cost of 1.  

###Exercise 8
#####A
```{r}
set.seed(1986)
train <- sample(1:(dim(OJ)[1]), 800)
OJ.train <- OJ[train,]
OJ.test <- OJ[-train,]
```

#####B & C
```{r}
OJ.model <- svm(Purchase ~., data = OJ.train, kernel = "linear", cost = 0.01)
summary(OJ.model)
train.pred <- predict(OJ.model, newdata = OJ.train)
sum(train.pred==OJ.train$Purchase)/800
test.pred <- predict(OJ.model, newdata = OJ.test)
sum(test.pred==OJ.test$Purchase)/270

```

The train accuracy is 83.4%, and the test accuracy is 83%.

#####D & E
```{r}
set.seed(347)
tune.linear <- tune(svm, Purchase ~., data=OJ.train,kernel="linear", ranges=list(cost=c(0.01, 0.1, 1,5,10)))
summary(tune.linear)
test.best <- predict(tune.linear$best.model, newdata = OJ.test)
sum(test.best==OJ.test$Purchase)/270
```
A cost of 1 minimizes the CV error.  The training error was 16.75%, while the test accuracy is 82.6%.

#####F
```{r}
set.seed(348)
tune.radial <- tune(svm, Purchase ~., data=OJ.train,kernel="radial", ranges=list(cost=c(0.01, 0.1, 1,5,10)))
summary(tune.radial)
radial.best <- predict(tune.radial$best.model, newdata = OJ.test)
sum(radial.best==OJ.test$Purchase)/270
```

The training accuracy is 83.125%, while the test accuracy is 80%.  The CV error is minimized for cost = 1.

#####F
```{r}
set.seed(348)
tune.poly <- tune(svm, Purchase ~., data=OJ.train,kernel="poly", ranges=list(cost=c(0.01, 0.1, 1,5,10), degree = c(2)))
summary(tune.poly)
poly.best <- predict(tune.poly$best.model, newdata = OJ.test)
sum(poly.best==OJ.test$Purchase)/270
```
The train accuracy is 82.75%, while the test accuracy is 80.4%.

#####G
The linear model seems to work the best.